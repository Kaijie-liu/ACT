#!/usr/bin/env python3
#===- act/pipeline/confignet_io.py - ConfigNet JSONL IO ---------------====#
# ACT: Abstract Constraint Transformer
# Copyright (C) 2025– ACT Team
#
# Licensed under the GNU Affero General Public License v3.0 or later (AGPLv3+).
# Distributed without any warranty; see <http://www.gnu.org/licenses/>.
#===---------------------------------------------------------------------===#
#
# Purpose:
#   ConfigNet IO utilities.
#   - JSONL v2 schema + record helpers
#   - canonical hashing + run_id
#   - examples_config materialization helpers (generated section + nets JSON)
#
#===---------------------------------------------------------------------===#

from __future__ import annotations

import hashlib
import json
import subprocess
import time
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Set

import torch
import yaml

from act.back_end.serialization.serialization import NetSerializer

# Marker prefix for networks generated by Confignet when materializing into
# examples_config.yaml. Keeping it stable lets us cleanly wipe/refresh the
# generated section on each run without touching hand-authored entries.
CONFIGNET_PREFIX = "cfg_seed"

# Default targets used by ModelFactory/validate_verifier
DEFAULT_EXAMPLES_CONFIG = "act/back_end/examples/examples_config.yaml"
DEFAULT_NETS_DIR = "act/back_end/examples/nets"

SCHEMA_VERSION_V2 = "confignet_l1l2_v2"


def _canonical_dumps(obj: Any) -> str:
    """Stable JSON serialization with sorted keys and compact separators."""
    return json.dumps(obj, sort_keys=True, separators=(",", ":"), ensure_ascii=True)


def canonical_hash(obj: Any) -> str:
    """SHA256 hash of canonical JSON representation."""
    data = _canonical_dumps(obj).encode("utf-8")
    return hashlib.sha256(data).hexdigest()


def _strip_keys(obj: Any, ignore_keys: Set[str]) -> Any:
    if isinstance(obj, dict):
        return {k: _strip_keys(v, ignore_keys) for k, v in obj.items() if k not in ignore_keys}
    if isinstance(obj, list):
        return [_strip_keys(v, ignore_keys) for v in obj]
    return obj


def canonical_hash_obj(obj: Dict[str, Any], *, ignore_keys: Optional[Set[str]] = None) -> str:
    """
    Canonical hash with optional key filtering for non-deterministic fields.
    """
    ignore = ignore_keys or {"timing", "created_at_utc", "timestamp"}
    stripped = _strip_keys(obj, ignore)
    return canonical_hash(stripped)


def compute_run_id(run_meta: Dict[str, Any]) -> str:
    """Compute deterministic run_id from run_meta."""
    return canonical_hash_obj(run_meta, ignore_keys=set())


def current_git_sha() -> str:
    """Return current git SHA (or 'unknown' if unavailable)."""
    try:
        out = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            check=True,
            capture_output=True,
            text=True,
        )
        sha = out.stdout.strip()
        return sha if sha else "unknown"
    except Exception:
        return "unknown"


def write_jsonl_records(path: str, records: Iterable[Dict[str, Any]], sort_keys: bool = False) -> None:
    """Write JSONL records to path, creating parent directories if needed."""
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("w", encoding="utf-8") as f:
        for rec in records:
            f.write(json.dumps(rec, sort_keys=sort_keys, ensure_ascii=True))
            f.write("\n")


def write_record_jsonl(path: str, record: Dict[str, Any]) -> None:
    """Append a single JSONL record to path."""
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=True))
        f.write("\n")


def read_jsonl(path: str) -> list[Dict[str, Any]]:
    """Read JSONL records from path."""
    out: list[Dict[str, Any]] = []
    with Path(path).open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            out.append(json.loads(line))
    return out


# ---------------------------------------------------------------------------
# examples_config.yaml helpers (Confignet → examples_config)
# ---------------------------------------------------------------------------


def _strip_generated_section(lines: List[str]) -> List[str]:
    """
    Remove previously generated Confignet section delimited by START/END markers.
    Safe for idempotent rewrites of examples_config.yaml.
    """
    start = None
    end = None
    for i, ln in enumerate(lines):
        if "==== GENERATED BY CONFIGNET START ====" in ln:
            start = i
        if "==== GENERATED BY CONFIGNET END ====" in ln:
            end = i
    if start is not None and end is not None and end >= start:
        return lines[:start] + lines[end + 1 :]
    return lines


def _yaml_dump_block(data: Dict[str, Any], indent: int = 2) -> str:
    """Dump YAML with fixed indent for embedding inside networks map."""
    return yaml.dump(data, sort_keys=False, default_flow_style=False, indent=indent)


def render_confignet_entries(entries: Dict[str, Dict[str, Any]]) -> str:
    """
    Render generated networks into a YAML block with begin/end markers.

    Args:
        entries: mapping name -> network spec (already aligned with examples_config schema)
    """
    if not entries:
        return ""
    header = "  # ==== GENERATED BY CONFIGNET START ====\n"
    body_parts = []
    for name, spec in entries.items():
        # Each entry is indented under networks:
        dumped = _yaml_dump_block({name: spec}, indent=2)
        # yaml.dump already ends with newline
        indented = "\n".join(["  " + ln if ln.strip() else ln for ln in dumped.splitlines()])
        body_parts.append(indented + "\n")
    footer = "  # ==== GENERATED BY CONFIGNET END ====\n"
    return header + "".join(body_parts) + footer


def write_confignet_entries_to_examples_config(
    *,
    entries: Dict[str, Dict[str, Any]],
    config_path: str = DEFAULT_EXAMPLES_CONFIG,
) -> None:
    """
    Append generated Confignet entries into examples_config.yaml under networks.

    Strategy:
    - Read existing file as lines
    - Remove previous generated section (markers)
    - Inject new section right before EOF (still inside networks map)
    - Generated names are expected to have CONFIGNET_PREFIX to avoid clashes
    """
    path = Path(config_path)
    text = path.read_text(encoding="utf-8")
    lines = text.splitlines()
    lines = _strip_generated_section(lines)

    # naive approach: append at end. examples_config.yaml has top-level 'networks:' already.
    # Ensure there is a trailing newline
    if lines and lines[-1].strip() != "":
        lines.append("")

    block = render_confignet_entries(entries)
    if block:
        lines.append(block.rstrip("\n"))

    path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def load_examples_config(path: str = DEFAULT_EXAMPLES_CONFIG) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def append_nets_json(
    *,
    nets: Dict[str, Any],
    nets_dir: str = DEFAULT_NETS_DIR,
) -> None:
    """Write ACT Net JSON files for generated entries (names must match)."""
    out_dir = Path(nets_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    for name, net in nets.items():
        path = out_dir / f"{name}.json"
        payload = NetSerializer.serialize_net(net)
        for layer in payload.get("act_net", {}).get("layers", []):
            if layer.get("kind") == "INPUT":
                params = layer.get("params", {})
                params.pop("labeled_input", None)
        sanitized = _to_basic(payload)
        path.write_text(json.dumps(sanitized, ensure_ascii=True, indent=2), encoding="utf-8")


# ---------------------------------------------------------------------------
# Conversion helpers: ACT Net -> examples_config.yaml entry
# ---------------------------------------------------------------------------


def _to_basic(obj: Any) -> Any:
    """Convert tensors/Enums to JSON-serializable primitives."""
    if obj is None or isinstance(obj, (bool, int, float, str)):
        return obj
    if isinstance(obj, (list, tuple)):
        return [_to_basic(x) for x in obj]
    if isinstance(obj, dict):
        return {str(k): _to_basic(v) for k, v in obj.items()}
    if isinstance(obj, torch.Tensor):
        t = obj.detach().cpu()
        if t.numel() == 1:
            return float(t.item())
        return t.tolist()
    if hasattr(obj, "value"):
        return getattr(obj, "value")
    return str(obj)


def act_net_to_examples_entry(
    instance: Any,
    act_net: Any,
    *,
    description: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Convert an ACT Net into an examples_config.yaml network entry.
    """
    def _sanitize_params(kind: str, params: Dict[str, Any]) -> Dict[str, Any]:
        if not params:
            return {}
        if kind == "ASSERT":
            return _to_basic(params)
        cleaned: Dict[str, Any] = {}
        for key, val in params.items():
            if torch.is_tensor(val):
                continue
            cleaned[key] = _to_basic(val)
        return cleaned

    layers_spec: List[Dict[str, Any]] = []
    for layer in getattr(act_net, "layers", []):
        kind = str(getattr(layer, "kind", ""))
        params = _sanitize_params(kind, getattr(layer, "params", {}))
        layers_spec.append(
            {
                "kind": kind,
                "params": params,
                "meta": _to_basic(getattr(layer, "meta", {})),
            }
        )

    arch = getattr(instance.family, "value", getattr(instance, "family", "unknown"))
    if arch == "cnn2d":
        arch = "cnn"

    return {
        "description": description or f"Generated by Confignet ({instance.instance_id})",
        "architecture_type": arch,
        "input_shape": list(instance.model_cfg.input_shape),
        "layers": layers_spec,
    }


def make_record(payload: Dict[str, Any], include_timestamp: bool = True) -> Dict[str, Any]:
    """
    Attach stable hash + git SHA (+ optional timestamp) to a payload.

    Note: hash is computed ONLY from payload (not from git_sha/timestamp).
    """
    rec: Dict[str, Any] = dict(payload)
    rec["hash"] = canonical_hash(payload)
    rec["git_sha"] = current_git_sha()
    if include_timestamp:
        rec["timestamp"] = time.time()
    return rec


def tensor_digest(t: torch.Tensor, *, max_inline_numel: int = 64) -> Dict[str, Any]:
    """
    Serialize a tensor with stable metadata + SHA256 digest.

    If numel <= max_inline_numel, inline values to aid debugging.
    """
    if not torch.is_tensor(t):
        raise TypeError(f"tensor_digest expects torch.Tensor, got {type(t)}")
    t_cpu = t.detach().cpu().contiguous()
    raw = t_cpu.numpy().tobytes()
    digest = hashlib.sha256(raw).hexdigest()
    out: Dict[str, Any] = {
        "shape": list(t_cpu.shape),
        "dtype": str(t_cpu.dtype),
        "device": str(t_cpu.device),
        "sha256": digest,
    }
    if t_cpu.numel() <= int(max_inline_numel):
        out["values"] = t_cpu.tolist()
    return out


def canonicalize_record_v2(
    record: Dict[str, Any],
    *,
    ignore_keys: Optional[Set[str]] = None,
) -> Dict[str, Any]:
    """
    Return a canonicalized copy of a v2 record, dropping non-deterministic keys.
    """
    ignore = ignore_keys or {"timing", "created_at_utc", "timestamp"}
    stripped = _strip_keys(record, ignore)
    if not isinstance(stripped, dict):
        raise TypeError("canonicalize_record_v2 expects a dict record")
    solver = stripped.get("solver")
    if isinstance(solver, dict):
        solver.pop("time_sec", None)
        solver.pop("details", None)
    return stripped


def summarize_jsonl_v2(path: str) -> Dict[str, Any]:
    """
    Summarize v2 JSONL records for quick aggregation in tests or debugging.
    """
    records = read_jsonl(path)
    counts = {"PASS": 0, "FAILED": 0, "ERROR": 0}
    by_reason: Dict[str, int] = {}
    checks_total = 0
    violations_total = 0
    worst_gap_max = 0.0

    for rec in records:
        final = rec.get("final", {})
        verdict = final.get("final_verdict")
        if verdict in counts:
            counts[verdict] += 1
        reason = final.get("reason", "unknown")
        by_reason[reason] = by_reason.get(reason, 0) + 1

        l2 = rec.get("l2", {})
        checks_total += int(l2.get("checks_total", 0) or 0)
        violations_total += int(l2.get("violations_total", 0) or 0)
        gap = l2.get("worst_gap", 0.0)
        if gap is None:
            gap = 0.0
        worst_gap_max = max(worst_gap_max, float(gap))

    return {
        "records": len(records),
        "pass": counts["PASS"],
        "failed": counts["FAILED"],
        "error": counts["ERROR"],
        "by_reason": by_reason,
        "checks_total": int(checks_total),
        "violations_total": int(violations_total),
        "worst_gap_max": float(worst_gap_max),
    }


# --------------------------
# JSONL v2 schema
# --------------------------

def build_record_v2(
    *,
    run_meta: Dict[str, Any],
    seeds: Dict[str, Any],
    instance: Dict[str, Any],
    l1: Dict[str, Any],
    l2: Dict[str, Any],
    final: Dict[str, Any],
    solver: Optional[Dict[str, Any]] = None,
    timing: Optional[Dict[str, Any]] = None,
    errors: Optional[List[str]] = None,
    warnings: Optional[List[str]] = None,
    created_at_utc: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Build a v2 JSONL record from component dicts.
    """
    record: Dict[str, Any] = {
        "schema_version": SCHEMA_VERSION_V2,
        "run_id": compute_run_id(run_meta),
        "run_meta": dict(run_meta),
        "seeds": dict(seeds),
        "instance": dict(instance),
        "l1": dict(l1),
        "l2": dict(l2),
        "solver": dict(solver)
        if solver is not None
        else {
            "status": "NOT_RUN",
            "solver_name": "NOT_RUN",
            "verdict": "NOT_RUN",
            "time_sec": None,
            "errors": [],
            "warnings": [],
            "details": {},
        },
        "final": dict(final),
    }
    if timing is not None:
        record["timing"] = dict(timing)
    if errors:
        record["errors"] = list(errors)
    if warnings:
        record["warnings"] = list(warnings)
    if created_at_utc is not None:
        record["created_at_utc"] = str(created_at_utc)

    record["canonical_hash"] = canonical_hash_obj(record)
    return record


def _require_key(d: Dict[str, Any], key: str) -> Any:
    assert key in d, f"missing key: {key}"
    return d[key]


def _require_type(val: Any, typ, name: str) -> None:
    assert isinstance(val, typ), f"{name} must be {typ}, got {type(val)}"


def validate_record_v2(record: Dict[str, Any]) -> None:
    """
    Validate required fields and types for schema v2.
    """
    _require_type(record, dict, "record")
    assert record.get("schema_version") == SCHEMA_VERSION_V2, "invalid schema_version"
    _require_type(_require_key(record, "run_id"), str, "run_id")
    _require_type(_require_key(record, "canonical_hash"), str, "canonical_hash")

    run_meta = _require_key(record, "run_meta")
    _require_type(run_meta, dict, "run_meta")
    for key, typ in (
        ("seed_root", int),
        ("instances", int),
        ("samples", int),
        ("tf_mode", str),
        ("strict", bool),
        ("device", str),
        ("dtype", str),
        ("atol", float),
        ("rtol", float),
        ("topk", int),
    ):
        _require_type(_require_key(run_meta, key), typ, f"run_meta.{key}")

    seeds = _require_key(record, "seeds")
    _require_type(seeds, dict, "seeds")
    for key in ("seed_root", "seed_instance", "seed_inputs"):
        _require_type(_require_key(seeds, key), int, f"seeds.{key}")

    instance = _require_key(record, "instance")
    _require_type(instance, dict, "instance")
    for key, typ in (
        ("net_family", str),
        ("arch", dict),
        ("spec", dict),
        ("input_shape", list),
        ("eps", (int, float)),
    ):
        _require_type(_require_key(instance, key), typ, f"instance.{key}")

    l1 = _require_key(record, "l1")
    _require_type(l1, dict, "l1")
    for key, typ in (
        ("status", str),
        ("counterexample_found", bool),
        ("verifier_status", str),
        ("policy_applied", bool),
        ("final_verdict_after_policy", str),
    ):
        _require_type(_require_key(l1, key), typ, f"l1.{key}")

    l1_status = l1["status"]
    assert l1_status in ("PASSED", "FAILED", "ERROR", "INCONCLUSIVE", "ACCEPTABLE"), "invalid l1.status"

    l2 = _require_key(record, "l2")
    _require_type(l2, dict, "l2")
    for key, typ in (
        ("status", str),
        ("tf_mode", str),
        ("samples", int),
        ("checks_total", int),
        ("violations_total", int),
        ("worst_gap", (int, float, type(None))),
        ("topk", list),
        ("layerwise_stats", list),
    ):
        _require_type(_require_key(l2, key), typ, f"l2.{key}")
    l2_status = l2["status"]
    assert l2_status in ("PASSED", "FAILED", "ERROR"), "invalid l2.status"

    final = _require_key(record, "final")
    _require_type(final, dict, "final")
    for key, typ in (
        ("final_verdict", str),
        ("exit_code", int),
        ("reason", str),
    ):
        _require_type(_require_key(final, key), typ, f"final.{key}")

    solver = _require_key(record, "solver")
    _require_type(solver, dict, "solver")
    for key, typ in (
        ("status", str),
        ("solver_name", str),
        ("verdict", str),
        ("time_sec", (int, float, type(None))),
        ("errors", list),
        ("warnings", list),
        ("details", dict),
    ):
        _require_type(_require_key(solver, key), typ, f"solver.{key}")
    assert final["final_verdict"] in ("PASS", "FAILED", "ERROR"), "invalid final.final_verdict"
