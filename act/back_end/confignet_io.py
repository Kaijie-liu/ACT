#!/usr/bin/env python3
#===- act/back_end/confignet_io.py - ConfigNet IO ---------------------====#
# ACT: Abstract Constraint Transformer
# Copyright (C) 2025– ACT Team
#
# Licensed under the GNU Affero General Public License v3.0 or later (AGPLv3+).
# Distributed without any warranty; see <http://www.gnu.org/licenses/>.
#===---------------------------------------------------------------------===#
#
# Purpose:
#   ConfigNet IO utilities.
#   - examples_config materialization helpers (generated entries)
#
#===---------------------------------------------------------------------===#

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import random

import yaml

from act.back_end.confignet_spec import (
    CNN2DConfig,
    InstanceSpec,
    MLPConfig,
    ModelFamily,
    _to_basic,
)

# Prefix for networks generated by Confignet when materializing into
# examples_config.yaml. Keeping it stable lets us cleanly wipe/refresh the
# generated entries on each run without touching hand-authored entries.
CONFIGNET_PREFIX = "cfg_seed"

# Default target used by examples config
DEFAULT_EXAMPLES_CONFIG = "act/back_end/examples/examples_config.yaml"

# NOTE: JSONL record helpers and run-id hashing were removed. Confignet
# now only materializes YAML entries.
# ---------------------------------------------------------------------------
# examples_config.yaml helpers (Confignet → examples_config)
# ---------------------------------------------------------------------------


def write_confignet_entries_to_examples_config(
    *,
    entries: Dict[str, Dict[str, Any]],
    config_path: str = DEFAULT_EXAMPLES_CONFIG,
    prefix: str = CONFIGNET_PREFIX,
) -> None:
    """
    Replace the generated Confignet entries inside examples_config.yaml.

    Strategy:
    - Load YAML as a dict
    - Remove old generated entries by prefix
    - Insert new generated entries under networks
    """
    if not entries:
        return
    if any(not name.startswith(prefix) for name in entries):
        bad = sorted(name for name in entries if not name.startswith(prefix))
        raise ValueError(f"Confignet entries must start with '{prefix}': {bad}")

    path = Path(config_path)
    data = load_examples_config(str(path))
    networks = data.get("networks")
    if networks is None:
        networks = {}
        data["networks"] = networks
    if not isinstance(networks, dict):
        raise ValueError("examples_config.yaml 'networks' must be a mapping")

    for name in list(networks.keys()):
        if name.startswith(prefix):
            networks.pop(name)

    collisions = [name for name in entries if name in networks]
    if collisions:
        raise ValueError(f"Confignet entry names collide with existing networks: {collisions}")

    networks.update(entries)
    path.write_text(yaml.safe_dump(data, sort_keys=False), encoding="utf-8")


def load_examples_config(path: str = DEFAULT_EXAMPLES_CONFIG) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


# ---------------------------------------------------------------------------
# Conversion helpers: InstanceSpec -> examples_config.yaml entry
# ---------------------------------------------------------------------------


def _prod(shape: Tuple[int, ...]) -> int:
    p = 1
    for s in shape:
        p *= int(s)
    return p


def _ensure_batch1(shape: Tuple[int, ...]) -> Tuple[int, ...]:
    if len(shape) < 2:
        raise ValueError(f"input_shape must include batch dim, got {shape}")
    if int(shape[0]) != 1:
        raise ValueError(f"Confignet assumes batch=1, got {shape}")
    return tuple(int(x) for x in shape)


def _activation_kind(name: str) -> str:
    name = (name or "relu").lower()
    if name == "relu":
        return "RELU"
    if name == "tanh":
        return "TANH"
    if name == "sigmoid":
        return "SIGMOID"
    raise ValueError(f"Unsupported activation '{name}' for examples_config")


def _infer_conv2d_output_hw(
    H: int,
    W: int,
    kernel: int,
    stride: int,
    padding: int,
    dilation: int = 1,
) -> Tuple[int, int]:
    def out_dim(x: int) -> int:
        return int((x + 2 * padding - dilation * (kernel - 1) - 1) // stride + 1)

    return out_dim(H), out_dim(W)


def _infer_pool2d_output_hw(
    H: int,
    W: int,
    kernel: int,
    stride: int,
    padding: int = 0,
) -> Tuple[int, int]:
    def out_dim(x: int) -> int:
        return int((x + 2 * padding - (kernel - 1) - 1) // stride + 1)

    return out_dim(H), out_dim(W)


def _as_block_param(v: Any, i: int, n_blocks: int, name: str) -> int:
    if isinstance(v, int):
        return int(v)
    t = tuple(int(x) for x in v)
    if len(t) == 1:
        return int(t[0])
    if len(t) == n_blocks:
        return int(t[i])
    raise ValueError(f"{name} must be int or tuple of len 1 or len {n_blocks}, got len={len(t)}")


def _append_conv2d(
    layers: List[Dict[str, Any]],
    *,
    in_ch: int,
    out_ch: int,
    H: int,
    W: int,
    kernel: int,
    stride: int,
    padding: int,
) -> Tuple[int, int]:
    input_shape = [1, int(in_ch), int(H), int(W)]
    out_H, out_W = _infer_conv2d_output_hw(H, W, kernel=kernel, stride=stride, padding=padding)
    if out_H <= 0 or out_W <= 0:
        raise ValueError(f"Invalid CONV2D output shape: H={out_H}, W={out_W}")
    output_shape = [1, int(out_ch), int(out_H), int(out_W)]
    layers.append(
        {
            "kind": "CONV2D",
            "params": {},
            "meta": {
                "in_channels": int(in_ch),
                "out_channels": int(out_ch),
                "kernel_size": int(kernel),
                "stride": int(stride),
                "padding": int(padding),
                "input_shape": input_shape,
                "output_shape": output_shape,
            },
        }
    )
    return out_H, out_W


def _append_pool2d(
    layers: List[Dict[str, Any]],
    *,
    kind: str,
    in_ch: int,
    H: int,
    W: int,
    kernel: int,
    stride: int,
    padding: int = 0,
) -> Tuple[int, int]:
    input_shape = [1, int(in_ch), int(H), int(W)]
    out_H, out_W = _infer_pool2d_output_hw(H, W, kernel=kernel, stride=stride, padding=padding)
    if out_H <= 0 or out_W <= 0:
        raise ValueError(f"Invalid {kind} output shape: H={out_H}, W={out_W}")
    output_shape = [1, int(in_ch), int(out_H), int(out_W)]
    layers.append(
        {
            "kind": kind,
            "params": {},
            "meta": {
                "kernel_size": int(kernel),
                "stride": int(stride),
                "padding": int(padding),
                "input_shape": input_shape,
                "output_shape": output_shape,
            },
        }
    )
    return out_H, out_W


def _build_mlp_layers(
    layers: List[Dict[str, Any]],
    *,
    cfg: MLPConfig,
) -> None:
    shape = _ensure_batch1(tuple(cfg.input_shape))
    in_features = int(shape[1]) if len(shape) == 2 else _prod(shape[1:])

    if len(shape) > 2:
        layers.append({"kind": "FLATTEN", "params": {}, "meta": {"start_dim": 1}})

    act_kind = _activation_kind(cfg.activation)

    if cfg.variant == "plain":
        for h in cfg.hidden_sizes:
            layers.append(
                {
                    "kind": "DENSE",
                    "params": {},
                    "meta": {
                        "in_features": int(in_features),
                        "out_features": int(h),
                        "bias_enabled": bool(cfg.use_bias),
                    },
                }
            )
            layers.append({"kind": act_kind, "params": {}, "meta": {}})
            in_features = int(h)
    else:
        width = int(cfg.block_width or (cfg.hidden_sizes[0] if cfg.hidden_sizes else 64))
        layers.append(
            {
                "kind": "DENSE",
                "params": {},
                "meta": {
                    "in_features": int(in_features),
                    "out_features": int(width),
                    "bias_enabled": bool(cfg.use_bias),
                },
            }
        )
        layers.append({"kind": act_kind, "params": {}, "meta": {}})
        in_features = int(width)

        for _ in range(int(cfg.num_blocks)):
            layers.append(
                {
                    "kind": "DENSE",
                    "params": {},
                    "meta": {
                        "in_features": int(in_features),
                        "out_features": int(in_features),
                        "bias_enabled": bool(cfg.use_bias),
                    },
                }
            )
            layers.append({"kind": act_kind, "params": {}, "meta": {}})
            layers.append(
                {
                    "kind": "DENSE",
                    "params": {},
                    "meta": {
                        "in_features": int(in_features),
                        "out_features": int(in_features),
                        "bias_enabled": bool(cfg.use_bias),
                    },
                }
            )
            if cfg.post_block_activation:
                layers.append({"kind": act_kind, "params": {}, "meta": {}})

    layers.append(
        {
            "kind": "DENSE",
            "params": {},
            "meta": {
                "in_features": int(in_features),
                "out_features": int(cfg.num_classes),
                "bias_enabled": True,
            },
        }
    )


def _build_cnn_layers(
    layers: List[Dict[str, Any]],
    *,
    cfg: CNN2DConfig,
    rng: random.Random,
) -> None:
    shape = _ensure_batch1(tuple(cfg.input_shape))
    if len(shape) != 4:
        raise ValueError(f"CNN2D expects input_shape=(1,C,H,W), got {shape}")
    _, in_ch, H, W = shape
    in_ch = int(in_ch)
    act_kind = _activation_kind(cfg.activation)

    if cfg.variant == "plain":
        n_blocks = len(cfg.conv_channels)
        for i, out_ch in enumerate(cfg.conv_channels):
            k = _as_block_param(cfg.kernel_sizes, i, n_blocks, "kernel_sizes")
            s = _as_block_param(cfg.strides, i, n_blocks, "strides")
            p = _as_block_param(cfg.paddings, i, n_blocks, "paddings")

            H, W = _append_conv2d(
                layers,
                in_ch=in_ch,
                out_ch=int(out_ch),
                H=H,
                W=W,
                kernel=int(k),
                stride=int(s),
                padding=int(p),
            )
            layers.append({"kind": act_kind, "params": {}, "meta": {}})
            in_ch = int(out_ch)

            if cfg.use_maxpool:
                H, W = _append_pool2d(
                    layers,
                    kind="MAXPOOL2D",
                    in_ch=in_ch,
                    H=H,
                    W=W,
                    kernel=int(cfg.maxpool_kernel),
                    stride=int(cfg.maxpool_stride),
                    padding=0,
                )
        layers.append({"kind": "FLATTEN", "params": {}, "meta": {"start_dim": 1}})
        feat = int(in_ch * H * W)
        layers.append(
            {
                "kind": "DENSE",
                "params": {},
                "meta": {
                    "in_features": int(feat),
                    "out_features": int(cfg.fc_hidden),
                    "bias_enabled": True,
                },
            }
        )
        layers.append({"kind": act_kind, "params": {}, "meta": {}})
        layers.append(
            {
                "kind": "DENSE",
                "params": {},
                "meta": {
                    "in_features": int(cfg.fc_hidden),
                    "out_features": int(cfg.num_classes),
                    "bias_enabled": True,
                },
            }
        )
        return

    ch = int(cfg.base_channels)
    H, W = _append_conv2d(
        layers,
        in_ch=in_ch,
        out_ch=ch,
        H=H,
        W=W,
        kernel=3,
        stride=1,
        padding=1,
    )
    layers.append({"kind": act_kind, "params": {}, "meta": {}})

    for stage in range(int(cfg.stages)):
        if stage > 0:
            next_ch = min(64, ch * int(cfg.channel_mult))
            if cfg.downsample == "stride2_conv":
                H, W = _append_conv2d(
                    layers,
                    in_ch=ch,
                    out_ch=next_ch,
                    H=H,
                    W=W,
                    kernel=3,
                    stride=2,
                    padding=1,
                )
                layers.append({"kind": act_kind, "params": {}, "meta": {}})
                ch = next_ch
            else:
                pool_kind = "MAXPOOL2D" if cfg.downsample == "maxpool" else "AVGPOOL2D"
                H, W = _append_pool2d(
                    layers,
                    kind=pool_kind,
                    in_ch=ch,
                    H=H,
                    W=W,
                    kernel=2,
                    stride=2,
                    padding=0,
                )
                if next_ch != ch:
                    H, W = _append_conv2d(
                        layers,
                        in_ch=ch,
                        out_ch=next_ch,
                        H=H,
                        W=W,
                        kernel=1,
                        stride=1,
                        padding=0,
                    )
                    layers.append({"kind": act_kind, "params": {}, "meta": {}})
                    ch = next_ch

        for _ in range(int(cfg.blocks_per_stage)):
            make_double_conv = (rng.random() < float(cfg.double_conv_p))
            if make_double_conv:
                H, W = _append_conv2d(
                    layers,
                    in_ch=ch,
                    out_ch=ch,
                    H=H,
                    W=W,
                    kernel=3,
                    stride=1,
                    padding=1,
                )
                layers.append({"kind": act_kind, "params": {}, "meta": {}})
                H, W = _append_conv2d(
                    layers,
                    in_ch=ch,
                    out_ch=ch,
                    H=H,
                    W=W,
                    kernel=3,
                    stride=1,
                    padding=1,
                )
                layers.append({"kind": act_kind, "params": {}, "meta": {}})
            else:
                H, W = _append_conv2d(
                    layers,
                    in_ch=ch,
                    out_ch=ch,
                    H=H,
                    W=W,
                    kernel=3,
                    stride=1,
                    padding=1,
                )
                layers.append({"kind": act_kind, "params": {}, "meta": {}})

    while cfg.head_pool_to_1x1 and (H > 1 or W > 1):
        H, W = _append_pool2d(
            layers,
            kind="AVGPOOL2D",
            in_ch=ch,
            H=H,
            W=W,
            kernel=2,
            stride=2,
            padding=0,
        )
        if H <= 0 or W <= 0:
            raise ValueError("Invalid spatial dims after head pooling")

    layers.append({"kind": "FLATTEN", "params": {}, "meta": {"start_dim": 1}})
    feat = int(ch * H * W)
    layers.append(
        {
            "kind": "DENSE",
            "params": {},
            "meta": {
                "in_features": int(feat),
                "out_features": int(cfg.num_classes),
                "bias_enabled": True,
            },
        }
    )


def instance_to_examples_entry(
    instance: InstanceSpec,
    *,
    dtype: str = "torch.float64",
    description: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Convert a ConfigNet instance into an examples_config.yaml network entry.
    """
    model_cfg = instance.model_cfg
    input_shape = list(model_cfg.input_shape)
    num_classes = int(getattr(model_cfg, "num_classes", 2))

    layers: List[Dict[str, Any]] = []

    input_meta: Dict[str, Any] = {
        "shape": input_shape,
        "dtype": str(dtype),
        "desc": "Confignet input",
        "num_classes": num_classes,
        "value_range": list(instance.input_spec.value_range),
    }
    dataset_name = instance.meta.get("dataset_name")
    if dataset_name:
        input_meta["dataset_name"] = dataset_name

    layers.append({"kind": "INPUT", "params": {}, "meta": input_meta})

    in_kind = str(instance.input_spec.kind)
    spec_meta: Dict[str, Any] = {"kind": in_kind}
    if in_kind == "BOX":
        spec_meta["lb_val"] = float(instance.input_spec.lb_val if instance.input_spec.lb_val is not None else instance.input_spec.value_range[0])
        spec_meta["ub_val"] = float(instance.input_spec.ub_val if instance.input_spec.ub_val is not None else instance.input_spec.value_range[1])
    elif in_kind == "LINF_BALL":
        spec_meta["center_val"] = float(instance.input_spec.center_val if instance.input_spec.center_val is not None else sum(instance.input_spec.value_range) / 2.0)
        spec_meta["eps"] = float(instance.input_spec.eps if instance.input_spec.eps is not None else 0.0)
    else:
        raise ValueError(f"Input spec kind '{in_kind}' is not supported in examples_config")

    layers.append({"kind": "INPUT_SPEC", "params": {}, "meta": spec_meta})

    if instance.family == ModelFamily.MLP:
        assert isinstance(model_cfg, MLPConfig)
        _build_mlp_layers(layers, cfg=model_cfg)
        arch = "mlp"
    elif instance.family == ModelFamily.CNN2D:
        assert isinstance(model_cfg, CNN2DConfig)
        rng = random.Random(int(instance.seed))
        _build_cnn_layers(layers, cfg=model_cfg, rng=rng)
        arch = "cnn"
    else:
        raise ValueError(f"Unsupported model family: {instance.family}")

    out_kind = str(instance.output_spec.kind)
    out_meta: Dict[str, Any] = {"kind": out_kind}
    out_params: Dict[str, Any] = {}

    if out_kind == "TOP1_ROBUST":
        out_meta["y_true"] = int(instance.output_spec.y_true or 0)
    elif out_kind == "MARGIN_ROBUST":
        out_meta["y_true"] = int(instance.output_spec.y_true or 0)
        out_meta["margin"] = float(instance.output_spec.margin)
    elif out_kind == "LINEAR_LE":
        out_params["c"] = _to_basic(instance.output_spec.c) if instance.output_spec.c is not None else [1.0] * num_classes
        out_meta["d"] = float(instance.output_spec.d if instance.output_spec.d is not None else 0.0)
    elif out_kind == "RANGE":
        out_params["lb"] = _to_basic(instance.output_spec.lb) if instance.output_spec.lb is not None else [0.0] * num_classes
        out_params["ub"] = _to_basic(instance.output_spec.ub) if instance.output_spec.ub is not None else [0.0] * num_classes
    else:
        out_meta["y_true"] = int(instance.output_spec.y_true or 0)

    layers.append({"kind": "ASSERT", "params": out_params, "meta": out_meta})

    return {
        "description": description or f"Generated by Confignet ({instance.instance_id})",
        "architecture_type": arch,
        "input_shape": input_shape,
        "layers": layers,
    }
